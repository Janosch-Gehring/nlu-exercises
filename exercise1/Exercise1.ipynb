{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d2e015-d254-4037-bab0-1030f7c59baf",
   "metadata": {},
   "source": [
    "# Week 2 Exercise\n",
    "\n",
    "In this first exercise, we will learn about word embeddings. Here, you will:\n",
    "\n",
    "- Try out static embeddings\n",
    "- Try out contextual embeddings\n",
    "- Visualize embeddings in a 2D-Space\n",
    "\n",
    "As it is the first exercise, there is not that much programming work here, it is more about experimentation :)\n",
    "\n",
    "To start, install the requirements. You should really do this inside of a virtual environment, so create one first.\n",
    "\n",
    "If you are comfortable using conda, you can read the environment.yml file to create an environment.\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "If you want to use pip:\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you use VS Code, you can of course use that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2287dc5-e890-470a-9e12-10282aad91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check if your dependencies are installed.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0955a51-eae0-4efd-9460-35618a0a6bde",
   "metadata": {},
   "source": [
    "# Part 1: Static Embeddings\n",
    "\n",
    "First, let's look at static embeddings like Word2Vec and GloVe. \n",
    "\n",
    "Let's download some GloVe embeddings. GloVe is second-order, so if words ('word instances') appear in similar contexts (i.e. surrounded by the same words), they will be close together in the embedding space. \n",
    "\n",
    "We will work with relatively small 50-dimensional Glove-Embeddings for now.\n",
    "\n",
    "Run the code below to load the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2b0c4-5b35-496b-9758-86636d7a7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim import downloader\n",
    "glove = downloader.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"Glove loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9931f-8ab5-479f-9ea5-a63c946b2627",
   "metadata": {},
   "source": [
    "We can use cosine similarity to get a feel for how close words are in the embedding space.\n",
    "\n",
    "Try running this code. Also try replacing these word pairs with some of your own and calculating their similarity. Take note of cases where the similarity is unintuitively high or low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecbc08-dcec-4947-89a3-bf29b9006c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"mouse\", \"cat\"),\n",
    "    (\"mouse\", \"dog\"),\n",
    "    (\"mouse\", \"hamster\"),\n",
    "    (\"mouse\", \"hole\"),\n",
    "    (\"mouse\", \"cheese\"),\n",
    "    (\"mouse\", \"sewer\"),\n",
    "    (\"mouse\", \"computer\"),\n",
    "    (\"mouse\", \"airport\"),\n",
    "    (\"mouse\", \"politics\")\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    sim = cosine_similarity(glove[word1], glove[word2])\n",
    "    print(f\"Cosine similarity between '{word1}' and '{word2}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2b1dd-7516-4eb7-9e85-fdff53b8606c",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "\n",
    "Examine the cosine similarities. Are they as you would expect? Are there any with unintuitively high or low similarities, and what do those tell you about how second-order/static embeddings work and potential issues? Also try out other word pairs to make your points.\n",
    "\n",
    "### Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344e4a5-ae9a-4ab7-981a-63d6783558ad",
   "metadata": {},
   "source": [
    "# Part 2: Contextual Embeddings\n",
    "\n",
    "Popular embedding models like BERT are contextualized - words have different embeddings depending on the context.\n",
    "\n",
    "Let's load up BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f4262-aeed-4407-8612-0e846921b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf65e0e-3348-4ac6-852d-ee5484547d3c",
   "metadata": {},
   "source": [
    "Let's define a simple function for getting the embedding of a target word from a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63346935-9bb1-494b-8653-e22ca0df5317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_token_embedding(sentence, target_word):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state.squeeze(0) \n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
    "\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == target_word:\n",
    "            return last_hidden_state[idx]\n",
    "\n",
    "    raise ValueError(f\"Token '{target_word}' not found in: {tokens}\")\n",
    "\n",
    "get_token_embedding(\"penguins are cool\", \"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba99b2-54a4-420a-b6d3-060cee9eadcc",
   "metadata": {},
   "source": [
    "Just like with GloVe, we can also compute cosine similarities. Since they are context dependent, even the same words can have different embeddings (and thus, a cosine similarity other than 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e3513-57cd-4731-acf2-d30bd518851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = get_token_embedding(\"I feel great\", \"feel\")\n",
    "word2 = get_token_embedding(\"I feel good\", \"feel\")\n",
    "\n",
    "print(cosine_similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd975c-74e1-41a6-9d4d-ffd01537682f",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Play around with BERT and cosine similarities: \n",
    "1) Does the cosine similarity correlate with the similarity of context semantics, syntax, and word choice? When do vectors of the same word have very high or very low similarities?\n",
    "\n",
    "### Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adc314-5b5d-4a65-b81a-70a8fcd62ed1",
   "metadata": {},
   "source": [
    "# Part 3: Visualizing Embeddings\n",
    "\n",
    "Let's try to visualize BERT embeddings. PCA is a common algorithm to use for dimensionality reduction (sklearn.decomposition.PCA), but we will use TSNE here (sklearn.manifold.TSNE), which is optimized for visualization. \n",
    "\n",
    "## Task 3\n",
    "\n",
    "Make yourself familiar with the TSNE algorithm. Implement a function below that takes some array/list of embedding vectors as input and outputs reduced representations. Check out the scikit-learn documentation https://scikit-learn.org/stable/api/index.html for more information.\n",
    "\n",
    "Hint: Using scikit-learn, you can easily do this in 1 or 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b43178-5b02-43c1-8875-7aa4a167deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def reduce_dimensionality(vectors: list):\n",
    "    # TODO implement here\n",
    "    return []    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9698df8-0dc4-4fd6-8a9d-a08403b25198",
   "metadata": {},
   "source": [
    "Let's test if we can plot BERT embeddings with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a95f6-50d8-40c7-b927-06808d6f3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [\n",
    "    # [context, target word]\n",
    "    [\"I like books\", \"books\"],\n",
    "    [\"I like learning\", \"learning\"],\n",
    "    [\"I like music\", \"music\"],\n",
    "    [\"I like food\", \"food\"],\n",
    "    [\"I like cake\", \"cake\"],\n",
    "    [\"I like cookies\", \"cookies\"],\n",
    "    [\"I like rice\", \"rice\"],\n",
    "    [\"I like pancakes\", \"pancakes\"]\n",
    "]\n",
    "\n",
    "def plot_vectors(embeddings):\n",
    "    labels = [x[0] for x in embeddings]\n",
    "\n",
    "    vectors = []\n",
    "    for sentence, word in embeddings:\n",
    "        vectors.append(get_token_embedding(sentence, word))\n",
    "\n",
    "    vectors = reduce_dimensionality(vectors)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = vectors[i]\n",
    "        plt.scatter(x, y)\n",
    "        plt.text(x + 0.01, y + 0.01, label, fontsize=9)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_vectors(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6f3f-269e-4e3b-a9fb-43620268385d",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Let's get to the issue of homonymy/polysemy. When a word has multiple senses, the context of the word decides its meaning. So perhaps it is possible to see which contexts imply the same word sense using these visualizations.\n",
    "\n",
    "For example: Regarding the vector of the word \"bat\", perhaps the vectors with the contexts \"I have a baseball bat\" and \"I hit the ball with the bat\" are close together, while \"The bat flies out of the cave\" will be further away.\n",
    "\n",
    "Your Task: Choose one word with two or more senses (e.g.: key, run, set) and think of contexts that imply a word sense. Create a graphic using data points of the same word in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdffcb7a-89ad-4b16-ac63-e9020d3ea286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2671e-8ccb-4766-98b3-1250f4ef6f4d",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Interpret the graphic you just created. Can you see any tendencies regarding word senses? Hint: If you cannot see any, try adding more data points until you see clusters forming.\n",
    "\n",
    "### Your Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46532f-f788-443b-bfde-0fb68afe5989",
   "metadata": {},
   "source": [
    "# Additional Task\n",
    "\n",
    "If you have no experience with regular expressions, I recommend reading up on the basics. Regular expressions are very useful for extracting information from complex structures.\n",
    "\n",
    "You can read up on regular expressions here: https://docs.python.org/3/library/re.html\n",
    "\n",
    "With that in mind, try solving the following exercise. Write regular expressions to get the outputs to match the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Task 1\n",
    "document = \"\"\"\n",
    "a b c d e <f> g h i j k a b c d <e> f g h i j k\n",
    "\"\"\"\n",
    "match = re.findall(r\"your pattern here\", document)\n",
    "# Return the letters inside the <> brackets. \n",
    "# Expected Output:\n",
    "#  f, e\n",
    "print(match)\n",
    "\n",
    "# Task 2\n",
    "document = \"\"\"\n",
    "qualification well-known finger introduction high-spirited dataset long-term story container six-pack music notebook\n",
    "\"\"\"\n",
    "match = re.findall(r\"your pattern here\", document)\n",
    "# Return the words that include a hyphen (-)\n",
    "# Expected Output:\n",
    "# well-known, high-spirited, long-term, six-pack\n",
    "print(match)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
