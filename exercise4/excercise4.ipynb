{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b60f9e",
   "metadata": {},
   "source": [
    "# Week 5 Exercise\n",
    "\n",
    "In this exercise we will fine-tune a vanilla transformer on the Multi-NLI dataset and compare it to a already fine-tuned model.\n",
    "\n",
    "In this exercise, we will:\n",
    "\n",
    "- load the vanilla BART-large model as well as the Multi-NLI dataset from huggingface\n",
    "- define a tokenization function to encode our input\n",
    "- define a evaluation function to use during fine-tuning\n",
    "- fine-tune and evaluate the vanilla model\n",
    "- compare your fine-tuned model to an already existing one\n",
    "- analyze the pretrained model on a set of linguistic phenomena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirements (not that many this time)\n",
    "!pip install transformers torch datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728de2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import evaluate, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f0939",
   "metadata": {},
   "source": [
    "### Task 1: Fine-tune the vanilla BART-large model on MNLI\n",
    "\n",
    "Check out this page to understand how to fine-tune a model on huggingface: https://huggingface.co/docs/transformers/training\n",
    "\n",
    "First, let's load the model that we want to fine-tune. We will use BART-large (https://huggingface.co/facebook/bart-large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c544ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will use\n",
    "model_name = \"facebook/bart-large\"\n",
    "\n",
    "# TODO: load tokenizer and model here\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a908bfb",
   "metadata": {},
   "source": [
    "Now let's load the MNLI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNLI dataset\n",
    "dataset = load_dataset(\"multi_nli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa328ca5",
   "metadata": {},
   "source": [
    "Next, let's define a tokenization function to tokenize our input.\n",
    "\n",
    "Some advice on this:\n",
    "\n",
    "- use max_length=128 if memory is an issue\n",
    "- remember that your input consists of two components (premise and hypothesis), which you should give to the tokenizer at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the tokenization function\n",
    "def tokenize(examples):\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19515ca6",
   "metadata": {},
   "source": [
    "Now that we can tokenize the input, let's tokenize the dataset using the `map` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e666e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: encode dataset using the `tokenize` function\n",
    "\n",
    "# your code here\n",
    "\n",
    "encoded_dataset = \"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d939d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define smaller train and validation dataset for speed\n",
    "# note that we will test the model on the validation dataset in this assignment as we're not going to report the numbers\n",
    "train_mnli = encoded_dataset[\"train\"].select(range(20000))\n",
    "val_mnli = encoded_dataset[\"validation_matched\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402695f",
   "metadata": {},
   "source": [
    "We still need to define a metric to evaluate our model and also define a function that the trainer can use to evaluate the model. We'll use accuracy as our evaluation metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation metric\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2693623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define the function that the trainer can use to compute and report the metric\n",
    "# note that BART-large may return its logits in a tuple and the first element is the relevant logit\n",
    "# so: if logits istype tuple → logits = logits[0]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    # your code here\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define training arguments here \n",
    "\n",
    "# training_args = TrainingArguments(…)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define trainer here \n",
    "# hint: create a data collator (DataCollatorWithPadding) for dynamic padding, yields better performance\n",
    "#       if you do, set data_collator=data_collator in your Trainer\n",
    "\n",
    "# trainer = Trainer(…)\n",
    "\n",
    "# your code here \n",
    "\n",
    "trainer = \"placeholder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate fine-tuned model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82400298",
   "metadata": {},
   "source": [
    "### Task 2: Compare to an already fine-tuned model\n",
    "\n",
    "Now we want to compare your fine-tuned model to a already fine-tuned model that we can find on huggingface. As luck would have it, there is an already fine-tuned BART-large model for MNLI available, so let's compare our model to this one!\n",
    "\n",
    "You can find the model here: https://huggingface.co/facebook/bart-large-mnli\n",
    "\n",
    "First, load the model and tokenizer. See this blog post for help (Step 1): https://www.geeksforgeeks.org/deep-learning/how-to-use-hugging-face-pretrained-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load model and tokenizer\n",
    "# note: load the tokenizer with the same name as the previous one to make sure that this tokenizer will be used in your tokenize function from now on\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14edd30",
   "metadata": {},
   "source": [
    "Before we evaluate this model, let's check the label map (always a good idea to do that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe7a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('facebook/bart-large-mnli')\n",
    "print(config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e19503",
   "metadata": {},
   "source": [
    "And now let's check the MNLI dataset that we just used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffded8",
   "metadata": {},
   "source": [
    "Apparently, the label mapping was inverted in Facebook's fine-tuned BART-large model and conflicts with the label mapping of MNLI. Make sure to fix this when evaluating the model on MNLI!\n",
    "\n",
    "Perform inference on Facebook's fine-tuned model on MNLI's validation dataset (the same that you used to evaluate your fine-tuned model!). Evaluate using accuracy as a metric by collecting the number of correctly classified items in your validation set and dividing them by the number of items in the validation dataset. Check Step 2 from the blog post to see how you can use a pretrained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function that takes the premise and the hypothesis as well as the pretrained model as input and outputs the model's prediction \n",
    "\n",
    "# uncomment the lines below if you want to use GPU\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# facebook_model.to(device)\n",
    "\n",
    "# use this label map to make sure that the indices point to the same label for the dataset and the model\n",
    "# MNLI mapping: 0: Entailment,      1: Neutral,     2: Contradiction\n",
    "# BART mapping: 0: Contradiction,   1: Neutral,     2: Entailment\n",
    "label_map = {0: 2, 1: 1, 2: 0}\n",
    "\n",
    "def run_finetuned_model(premise, hypothesis, model):\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2786bdd0",
   "metadata": {},
   "source": [
    "Now use `run_finetuned_model` on `val_mnli` and evaluate using accuracy.\n",
    "\n",
    "Remember: accuracy is just `correctly_classified_samples` / `all_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f48e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: loop over the samples in `val_mnli` and get the prediction for each sample\n",
    "# evaluate using the accuracy measure\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b38a3ef",
   "metadata": {},
   "source": [
    "### Task 3: Analyse Linguistic Phenomena in MNLI\n",
    "\n",
    "In the paper that you read for this session, the authors analyze their data by looking at some linguistic phenomena that are potentially difficult for a NLI model. We will do the same here. Choose at least two phenomena from the paper that you want to investigate and test some premise-hypothesis pairs on the `facebook/bart-large-mnli` model. For each of you categories, come up with at least 5 samples. Test the model on your sample and share your observations below.\n",
    "\n",
    "The categories from the paper are the following (second paragraph of section 4.3 on p.1119 in the paper for the categories):\n",
    "\n",
    "- Quantifiers\n",
    "- Belief Verbs\n",
    "- Time Terms\n",
    "- Discourse Markers\n",
    "- Presupposition Triggers\n",
    "- Conditionals\n",
    "\n",
    "Additionally, come up with at least one other category that is not in the paper and investigate how well the model is doing on examples from this category (at least 5 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c943a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example, you don't have to use quantifiers but you may of course\n",
    "# TODO: add your samples\n",
    "quantifier_example = [(\"All teachers were dancing.\", \"Some people were dancing.\")] # entailment\n",
    "\n",
    "# label map because BART-large has inverted label assignment\n",
    "label_map = {0: \"Contradiction\", 1: \"Neutral\", 2: \"Entailment\"}\n",
    "\n",
    "\n",
    "for (premise, hypothesis) in quantifier_example:\n",
    "    prediction = run_finetuned_model(premise, hypothesis, \"facebook_model\") # change \"facebook_model\" to your model variable\n",
    "    print(f\"{'Premise:':12}{premise}\\n{'Hypothesis:':12}{hypothesis}\\n{'Prediction:':12}{label_map[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee99f8",
   "metadata": {},
   "source": [
    "\\# Your observations here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
