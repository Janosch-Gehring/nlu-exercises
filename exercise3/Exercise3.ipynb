{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12d920",
   "metadata": {},
   "source": [
    "# Week 4 Exercise\n",
    "\n",
    "In this exercise we will look into coreference resolution as well as discourse analysis. Here, you will:\n",
    "\n",
    "- Experiment with a coreference resolution model\n",
    "- Analyze the model's output on some Winograd Schema questions\n",
    "- Investigate the model regarding gender biases\n",
    "- Experiment with a RST parser\n",
    "- Apply coreference resolution on last week's problem\n",
    "\n",
    "\n",
    "To start, install the requirements (we need --no-deps here because we have one package which is giving us trouble otherwise).\n",
    "\n",
    "```\n",
    "pip install --no-deps -r requirements.txt\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15591528",
   "metadata": {},
   "source": [
    "# Part 1: Coreference Resolution\n",
    "\n",
    "First, let's load and try the LingMessCoref coreference model from the package *fastcoref* (https://pypi.org/project/fastcoref/).\n",
    "\n",
    "Check out *fastcoref*'s documentation and write a function which uses the LingMessCoref model on a sample. Then apply the function on the sentences in test_doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to patch the model (there is a version mismatch between the model and transformers but we can get around that with this patch)\n",
    "\n",
    "import transformers, logging\n",
    "\n",
    "# Patch AutoModel to force eager attention everywhere\n",
    "orig_auto_model = transformers.AutoModel.from_config\n",
    "\n",
    "def patched_auto_model(config, *args, **kwargs):\n",
    "    kwargs[\"attn_implementation\"] = \"eager\"\n",
    "    return orig_auto_model(config, *args, **kwargs)\n",
    "\n",
    "transformers.AutoModel.from_config = patched_auto_model\n",
    "\n",
    "# add this for cleaner output\n",
    "transformers.logging.set_verbosity_error()  # or 'warning' for less strict\n",
    "logging.getLogger(\"fastcoref\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c99fe6",
   "metadata": {},
   "source": [
    "### Task 1 (Part 1): Implement a function to run the LingMessCoref \n",
    "\n",
    "Use the model loaded from *fastcoref* in the next cell by means of the function run_coreference_resolution. Return either lists of strings or lists of tuples (the spans), depending on return_string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import FastCoref and load the LingMessCoref model\n",
    "from fastcoref import LingMessCoref\n",
    "\n",
    "def run_coreference_resolution(model: LingMessCoref, sample: str, return_string=True) -> list: \n",
    "    # TODO code to use the LingMessCoref model here\n",
    "    # hint: check the type hints to understand what the function's input and output should be\n",
    "\n",
    "    if return_string:\n",
    "        return # lists of strings\n",
    "    else:\n",
    "        return # lists of tuples\n",
    "\n",
    "# load model here\n",
    "model = LingMessCoref(device=\"cpu\")  # or \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bde312",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = [\"'I told Nathan to pick up his books on the way home', she said.\",\n",
    "            \"Polly bought herself a nice screw driver.\",\n",
    "            \"Tom telephoned Tim. He was worried.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code uses your implemented function to resolve the coreferences in test_doc\n",
    "# and prints both the text and the span clusters\n",
    "\n",
    "for sample in test_doc:\n",
    "    text_clusters = run_coreference_resolution(model, sample)\n",
    "    span_clusters = run_coreference_resolution(model, sample, return_string=False)\n",
    "\n",
    "print(text_clusters)\n",
    "print(span_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d247974",
   "metadata": {},
   "source": [
    "## 1.1 The Winograd Schema Challenge\n",
    "\n",
    "Now we'll have a look at some samples from the Winograd Schema Challenge. Experiment with the two versions of the samples and check, whether the coreference prediction changes. \n",
    "\n",
    "To do this, use run_coreference_resolution on each sample of a pair and compare the clusters that form. If the model resolves the coreference correctly, the clusters should change.\n",
    "\n",
    "For instance, consider this example:\n",
    "\n",
    "- The path to the lake was blocked, so we couldn't reach it.\n",
    "- The path to the lake was blocked, so we couldn't use it.\n",
    "\n",
    "We as humans generally have no problem understanding what \"it\" refers to in these contexts (*lake* and *path* respectively) even though only one word changed in the sentence. Let's see if the coreference model can also solve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_coreference_resolution(model, \"The path to the lake was blocked, so we couldn't reach it.\")\n",
    "run_coreference_resolution(model, \"The path to the lake was blocked, so we couldn't use it.\")\n",
    "\n",
    "# output should be:\n",
    "# \n",
    "# [['The path to the lake', 'it']]\n",
    "# [['The path to the lake', 'it']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80924abe",
   "metadata": {},
   "source": [
    "Apparently the model is not able to solve this one. Check out the other examples in winograd_schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "winograd_schema = [(\"The city council denied the demonstrators a permit because they feared violence.\", \"The city council denied the demonstrators a permit because they announced violence.\"),\n",
    "                    (\"Jane gave Joan candy because she was hungry.\", \"Jane gave Joan candy because she wasn't hungry.\"),\n",
    "                    (\"The scientists are studying three species of fish that have recently been found living in the Indian Ocean. They began two years ago.\", \"The scientists are studying three species of fish that have recently been found living in the Indian Ocean. They were discovered two years ago.\"), \n",
    "                    (\"Since it was raining, I carried the newspaper in my backpack to keep it dry.\", \"Since it was raining, I carried the newspaper over my backpack to keep it dry.\"),\n",
    "                    (\"Sam and Amy are passionately in love, but Amy's parents are unhappy about it, because they are snobs.\", \"Sam and Amy are passionately in love, but Amy's parents are unhappy about it, because they are fifteen.\"),\n",
    "                    (\"The dog chased the cat, which ran up a tree. It waited at the top.\", \"The dog chased the cat, which ran up a tree. It waited at the bottom.\"),\n",
    "                    (\"Fred is the only man still alive who remembers my great-grandfather. He was a remarkable man.\", \"Fred is the only man still alive who remembers my great-grandfather. He is a remarkable man.\"), \n",
    "                    (\"Sara borrowed the book from the library because she needs it for an article she is working on. She will read it when she gets home from work.\", \"Sara borrowed the book from the library because she needs it for an article she is working on. She will write it when she gets home from work.\"), \n",
    "                    (\"The trophy doesn’t fit into the brown suitcase because it’s too small.\", \"The trophy doesn’t fit into the brown suitcase because it’s too large.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b67882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use run_coreference_resolution with winograd schema samples here\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3934145",
   "metadata": {},
   "source": [
    "### Task 1 (Part 2): Analyze the Model's Behaviour\n",
    "\n",
    "Where does the model deviate from your expectations? Why do you think that is? Comment on your observations and share your thoughts.\n",
    "\n",
    "\\# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde851a7",
   "metadata": {},
   "source": [
    "## 1.2 Winobias\n",
    "\n",
    "Now let's look at two documents from the WinoBias dataset: \n",
    "\n",
    "- *pro_stereotyped_type1.txt*\n",
    "- *anti_stereotyped_type1.txt*\n",
    "\n",
    "The files contain sentences with a gold annotation for a coreference cluster which is either pro-stereotypical (e.g. *laywer* and *he*) or anti-stereotypical (e.g. *janitor* and *she*). The target coreference is marked by square brackets around the coreferents (e.g. [janitor] and [she]). The authors created this dataset to investigate potential gender biases in coreference models, by comparing the models' performance on these two sub-datasets.\n",
    "\n",
    "You will now investigate the LingMessCoref model by comparing the performance of the model in the two documents provided.\n",
    "\n",
    "\n",
    "### Task 2: Extract gold annotations\n",
    "\n",
    "You'll first have to extract the gold annotations from the two documents such that they can be compared to the model's predictions. To get the predictions, you have to remove the square brackets such that you can input the text into your run_coreference_resolution function. Second, you'll need some evaluation measures. You can use the metrics provided by https://github.com/tollefj/coreference-eval/tree/main. Use the detailed_score call for the Scorer class to get the CoNLL-2012 F1-score. Make sure to check what the correct input format is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO read files, extract gold annotations and clean text from \"[\" and \"]\"\n",
    "# note that extract_gold_annotations returns both the cleaned text and the gold clusters\n",
    "# it makes sense to do both as we're iterating over the lines anyway\n",
    "\n",
    "import re\n",
    "\n",
    "def read_file(filename: str) -> list:\n",
    "    # TODO your code here\n",
    "\n",
    "    return # list of document lines\n",
    "\n",
    "\n",
    "def extract_gold_annotations(file: list) -> tuple[list, list]:\n",
    "    # TODO your code here\n",
    "\n",
    "    # you can assume that the target refering expressions only occur once in each line\n",
    "    # use re.findall, re.sub and re.finditer to locate the target spans and clean the text from \"[\" and \"]\"\n",
    "    # you can find the target sequence using the following pattern: r\"\"\"\\[([\\w\\s]+)\\]\"\"\"\n",
    "\n",
    "    return # (list of clean lines, list of gold clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get coreference predictions (spans!) for all lines\n",
    "# make sure to store them in the same format as the gold annotations\n",
    "\n",
    "def get_coref_predictions(lines: list, model: LingMessCoref) -> list:\n",
    "    # TODO your code here\n",
    "\n",
    "    return # list of predicted clusters (spans!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7dc507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use the evaluation framework from https://github.com/tollefj/coreference-eval/tree/main\n",
    "# hint: for every sample you'll need to create a document and update the scorer to get the final score over all samples\n",
    "#       you can then use detailed_score on the final scorer just like shown in the README\n",
    "#\n",
    "#       --> for pred, gold in preds, gold\n",
    "#               create document\n",
    "#               update scorer\n",
    "# \n",
    "#           get detailed_score from scorer          \n",
    "\n",
    "from corefeval import Scorer, Document\n",
    "\n",
    "def evaluate_coref(gold_docs: list, pred_docs: list) -> float:\n",
    "    # TODO your code here\n",
    "\n",
    "    return # final eval score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the two gold files\n",
    "lines_anti = read_file(\"anti_stereotyped_type1.txt\")\n",
    "lines_pro = read_file(\"pro_stereotyped_type1.txt\")\n",
    "\n",
    "# extract gold annotation from the samples from the files and clean lines from \"[\" and \"]\"\n",
    "clean_lines_anti, gold_anti = extract_gold_annotations(lines_anti)\n",
    "clean_lines_pro, gold_pro = extract_gold_annotations(lines_pro)\n",
    "\n",
    "# get predicted coreference clusters for the samples from the files\n",
    "preds_anti = get_coref_predictions(clean_lines_anti, model)\n",
    "preds_pro = get_coref_predictions(clean_lines_pro, model)\n",
    "\n",
    "# calculate and print evaluation of coreference resolution\n",
    "print(f\"Evaluation Scores for ANTI-stereotypical data: {evaluate_coref(gold_anti, preds_anti)}\")\n",
    "print(f\"Evaluation Scores for PRO-stereotypical data: {evaluate_coref(gold_pro, preds_pro)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be212364",
   "metadata": {},
   "source": [
    "### Task 3: Evaluate and analyze gender bias\n",
    "\n",
    "Now that you've evaluated the model on both documents, what can you say about the model regarding gender bias?\n",
    "\n",
    "\\# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6ecf0",
   "metadata": {},
   "source": [
    "# Part 2: Discourse Analysis\n",
    "\n",
    "Now let's check out a discourse analysis tool. In Chapter 24 you already read about Rhetorical Structure Theory, so let's try out the RST parser from https://github.com/tchewik/isanlp_rst/tree/master\n",
    "\n",
    "Use the parser to parse the texts in test_stories and store the result for each story in a separate rs3 file. Using the function provided below, generate a png image and inspect the resulting tree. Play around with the stories and try to change some discourse connectives (changing \"because\" to \"although\" maybe?). See how that affects the parsed result and share your observations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stories = [\n",
    "    \"Jonas watered the fern every morning because he didn't know how much water it actually needed. However, the leaves turned yellow within days. He stopped watering it, and slowly, the color returned. That's how he realized that he had been watering it too much.\",\n",
    "    \"Mira brewed coffee before her meeting, but she got a call that lasted twenty minutes. By the time she returned, the cup was cold and uninviting. Still, she drank it, since the meeting had tired her.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_rst.parser import Parser\n",
    "\n",
    "# TODO load RST parser here\n",
    "def load_parser() -> Parser:\n",
    "\n",
    "    return # loaded parser\n",
    "\n",
    "\n",
    "# TODO use RST parser here\n",
    "def rst_parsing(parser: Parser, story: str, filename: str) -> None:\n",
    "\n",
    "    return # no return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a061be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import isanlp_rst\n",
    "\n",
    "def visualize_rst_tree(rs3_file: str) -> None:\n",
    "    # PNG\n",
    "    isanlp_rst.to_png(rs3_file, f\"{rs3_file}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74eacd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_parser = load_parser()\n",
    "\n",
    "rst_parsing(rst_parser, test_stories[0], \"first\")\n",
    "visualize_rst_tree(\"first\")\n",
    "\n",
    "rst_parsing(rst_parser, test_stories[1], \"second\")\n",
    "visualize_rst_tree(\"second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08655ca",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "What did you observe in the parsed tree when you changed the stories? Comment on your observations here.\n",
    "\n",
    "\\# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea86a4",
   "metadata": {},
   "source": [
    "## Bonus \n",
    "\n",
    "Now let's combine your work from last week with the coreference resolution you implemented here:\n",
    "\n",
    "Last week, we used the FrameNet parser to see, how GPT5 is used in different kinds of text. However, we only took into account the instances where GPT5 was mentioned by clearly associated mentions such as \"GPT-5\". We ignored pronouns and therefore only saw part of the picture. Now, we want to include the pronouns as well. We will use the coreference resolution model to find the coreference clusters associated with GPT5, substitute pronouns with obvious referring expressions in the texts and finally re-run the FrameNet parser to update our findings from last week.\n",
    "\n",
    "First, get the coreference clusters of GPT5 in the data (*texts_about_GPT-5.csv*) and substitute all of these referring expressions in the text with a clear name for GPT5 such that you can easily target them with your code from last week. \n",
    "\n",
    "For instance, this sentence:\n",
    "\n",
    "\"GPT-5 isn’t limited to text—**it** can work with images, audio, and video …\"\n",
    "\n",
    "should be changed to this sentence:\n",
    "\n",
    "\"GPT-5 isn’t limited to text—**GPT-5** can work with images, audio, and video …\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM PREVIOUS EXERCISE\n",
    "\n",
    "import csv\n",
    "\n",
    "samples = []\n",
    "with open('texts_about_GPT-5.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        samples.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coreference clusters for our samples\n",
    "\n",
    "clusters = []\n",
    "\n",
    "for sample in samples:\n",
    "    # sample[1] is the text\n",
    "    res_text = run_coreference_resolution(model, sample[1])\n",
    "    res_span = run_coreference_resolution(model, sample[1], return_string=False)\n",
    "    clusters.append((res_text, res_span))\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ced33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO substitute the pronouns of the relevant clusters in the text (e.g. with \"GPT5\")\n",
    "# you can of course simply subsitute all occurences in the cluster so that all your referring expressions are the same\n",
    "# \n",
    "# hint: when you start substituting, start from the end of the string as you're working with spans\n",
    "\n",
    "subbed_samples = []     # append samples with substituted pronouns to this list\n",
    "\n",
    "# hint: make sure that each substituted sample in the list consists of [<id>, <text>, <source type>, <link>]\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32145430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM PREVIOUS EXERCISE\n",
    "\n",
    "from frame_semantic_transformer import FrameSemanticTransformer\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    frame_transformer = FrameSemanticTransformer()\n",
    "\n",
    "    return frame_transformer.detect_frames(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM PREVIOUS EXERCISE\n",
    "\n",
    "frame_counts = {}\n",
    "\n",
    "for sample in subbed_samples:\n",
    "    index, text, domain, _ = sample\n",
    "    if domain not in frame_counts:\n",
    "        frame_counts[domain] = {}\n",
    "\n",
    "    for sentence in text.split(\".\"):  # or a better splitting algorithm\n",
    "        frame_representation = parse_sentence(sentence)\n",
    "        for frame in frame_representation.frames:\n",
    "            for element in frame.frame_elements:\n",
    "                # TODO: change this to fit your referents if needed\n",
    "                if \"GPT-5\" not in element.text:\n",
    "                    continue\n",
    "                if element.name not in frame_counts[domain]:\n",
    "                    frame_counts[domain][element.name] = 0\n",
    "                frame_counts[domain][element.name] += 1\n",
    "\n",
    "print(frame_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e193cdb",
   "metadata": {},
   "source": [
    "### Update your Observations\n",
    "\n",
    "Is the trend you observe now different from the insights from last week's exercise?\n",
    "\n",
    "\\# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
